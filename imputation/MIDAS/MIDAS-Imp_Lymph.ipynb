{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d1dc554-89b4-4b0e-aa68-818103090b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import yaml\n",
    "import sys\n",
    "import h5py\n",
    "import logging\n",
    "import scanpy as sc\n",
    "from os.path import join\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sps\n",
    "from sklearn.cluster import KMeans\n",
    "import gzip\n",
    "from scipy.io import mmread\n",
    "from pathlib import Path, PurePath\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "def wrap_warn_plot(adata, basis, color, **kwargs):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "        sc.pl.embedding(adata, basis=basis, color=color, **kwargs)\n",
    "\n",
    "def get_umap(ad, use_reps=[]):\n",
    "    for use_rep in use_reps:\n",
    "        umap_add_key = f'{use_rep}_umap'\n",
    "        sc.pp.neighbors(ad, use_rep=use_rep, n_neighbors=15)\n",
    "        sc.tl.umap(ad)\n",
    "        ad.obsm[umap_add_key] = ad.obsm['X_umap']\n",
    "    return ad\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "def split_ob(ads, ad_ref, ob='obs', key='emb2'):\n",
    "    len_ads = [_.n_obs for _ in ads]\n",
    "    if ob=='obsm':\n",
    "        split_obsms = np.split(ad_ref.obsm[key], np.cumsum(len_ads[:-1]))\n",
    "        for ad, v in zip(ads, split_obsms):\n",
    "            ad.obsm[key] = v\n",
    "    else:\n",
    "        split_obs = np.split(ad_ref.obs[key].to_list(), np.cumsum(len_ads[:-1]))\n",
    "        for ad, v in zip(ads, split_obs):\n",
    "            ad.obs[key] = v\n",
    "\n",
    "def eval_ads(ads, ref_key, src_key, exclude=[]):\n",
    "    aris = []\n",
    "    for ad in ads:\n",
    "        _mask = ~ad.obs[ref_key].isin(exclude)\n",
    "        gt = ad.obs[ref_key].values[_mask]\n",
    "        pred = ad.obs[src_key].values[_mask]\n",
    "        aris.append(adjusted_rand_score(pred, gt))\n",
    "    return aris\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "def search_louvain(ad, use_rep, n_neighbors=15, n_clusters=5):\n",
    "    sc.pp.neighbors(ad, n_neighbors=n_neighbors, use_rep=use_rep)\n",
    "    rs = np.arange(0.1, 1.0, 0.1)\n",
    "    n_cs = []\n",
    "    for r in rs:\n",
    "        sc.tl.louvain(ad, resolution=r, key_added=f'r={r}')\n",
    "        n_cs.append(ad.obs[f'r={r}'].nunique())\n",
    "    n_cs = np.array(n_cs)\n",
    "    if (n_cs==n_clusters).sum() >= 1:\n",
    "        ri = np.where(n_cs==n_clusters)[0][0]\n",
    "        ad.obs['louvain_k'] = ad.obs[f'r={rs[ri]}'].to_list()\n",
    "    else:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(ad.obsm[use_rep])\n",
    "        ad.obs['louvain_k'] = kmeans.labels_.astype('str')\n",
    "\n",
    "import json\n",
    "import copy\n",
    "from matplotlib.image import imread\n",
    "def load_spatial(path, adata, library_id='0'):\n",
    "    tissue_positions_file = join(path, \"tissue_positions.csv\")\n",
    "    files = dict(\n",
    "        tissue_positions_file=tissue_positions_file,\n",
    "        scalefactors_json_file=join(path, \"scalefactors_json.json\"),\n",
    "        hires_image=join(path, \"tissue_hires_image.png\"),\n",
    "        lowres_image=join(path, \"tissue_lowres_image.png\"),\n",
    "    )\n",
    "    \n",
    "    adata.uns[\"spatial\"] = dict()\n",
    "    adata.uns[\"spatial\"][library_id] = dict()\n",
    "    adata.uns[\"spatial\"][library_id][\"images\"] = dict()\n",
    "    for res in [\"hires\", \"lowres\"]:\n",
    "        try:\n",
    "            adata.uns[\"spatial\"][library_id][\"images\"][res] = imread(\n",
    "                str(files[f\"{res}_image\"])\n",
    "            )\n",
    "        except Exception:\n",
    "            raise OSError(f\"Could not find '{res}_image'\")\n",
    "\n",
    "    # read json scalefactors\n",
    "    adata.uns[\"spatial\"][library_id][\"scalefactors\"] = json.loads(\n",
    "        Path(files[\"scalefactors_json_file\"]).read_bytes()\n",
    "    )\n",
    "\n",
    "    # adata.uns[\"spatial\"][library_id][\"metadata\"] = {\n",
    "    #     k: (str(attrs[k], \"utf-8\") if isinstance(attrs[k], bytes) else attrs[k])\n",
    "    #     for k in (\"chemistry_description\", \"software_version\")\n",
    "    #     if k in attrs\n",
    "    # }\n",
    "\n",
    "    # read coordinates\n",
    "    positions = pd.read_csv(\n",
    "        files[\"tissue_positions_file\"],\n",
    "        header=0 if Path(tissue_positions_file).name == \"tissue_positions.csv\" else None,\n",
    "        index_col=0,\n",
    "    )\n",
    "    positions.columns = [\n",
    "        \"in_tissue\",\n",
    "        \"array_row\",\n",
    "        \"array_col\",\n",
    "        \"pxl_col_in_fullres\",\n",
    "        \"pxl_row_in_fullres\",\n",
    "    ]\n",
    "    # print(positions.head())\n",
    "\n",
    "    adata.obs = adata.obs.join(positions, how=\"left\")\n",
    "\n",
    "    adata.obsm[\"spatial\"] = adata.obs[\n",
    "        [\"pxl_row_in_fullres\", \"pxl_col_in_fullres\"]\n",
    "    ].to_numpy()\n",
    "   \n",
    "    adata.obs.drop(\n",
    "        columns=[\"pxl_row_in_fullres\", \"pxl_col_in_fullres\"],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "from scib.metrics import lisi\n",
    "def eval_lisi(\n",
    "        adata,\n",
    "        batch_keys=['domain', 'batch'],\n",
    "        label_keys = ['gt'],\n",
    "        use_rep='X_emb', use_neighbors=False,\n",
    "    ):\n",
    "    res = {}\n",
    "    for key in batch_keys:\n",
    "        adata.obs[key] = adata.obs[key].astype('category')\n",
    "\n",
    "        _lisi = lisi.ilisi_graph(\n",
    "            adata,\n",
    "            key,\n",
    "            'embed' if not use_neighbors else 'knn',\n",
    "            use_rep=use_rep,\n",
    "            k0=90,\n",
    "            subsample=None,\n",
    "            scale=True,\n",
    "            n_cores=1,\n",
    "            verbose=False,\n",
    "        )\n",
    "        res[key+'_iLISI'] = _lisi\n",
    "    for key in label_keys:\n",
    "        adata.obs[key] = adata.obs[key].astype('category')\n",
    "\n",
    "        _lisi = lisi.clisi_graph(\n",
    "            adata,\n",
    "            key,\n",
    "            'embed' if not use_neighbors else 'knn',\n",
    "            use_rep=use_rep,\n",
    "            batch_key=None,\n",
    "            k0=90,\n",
    "            subsample=None,\n",
    "            scale=True,\n",
    "            n_cores=1,\n",
    "            verbose=False,\n",
    "        )\n",
    "        res[key+'_cLISI'] = _lisi\n",
    "    df = pd.DataFrame.from_dict(res, orient='index').T\n",
    "    # df.columns = [_+'_LISI' for _ in df.columns]\n",
    "    return df\n",
    "\n",
    "os.environ['R_HOME'] = '/disco_500t/xuhua/miniforge3/envs/Seurat5/lib/R'\n",
    "os.environ['R_USER'] = '/disco_500t/xuhua/miniforge3/envs/Seurat5/lib/python3.8/site-packages/rpy2'\n",
    "def mclust_R(adata, num_cluster, modelNames='EEE', used_obsm='STAGATE', random_seed=2020):\n",
    "    np.random.seed(random_seed)\n",
    "    import rpy2.robjects as robjects\n",
    "    robjects.r.library(\"mclust\")\n",
    "\n",
    "    import rpy2.robjects.numpy2ri\n",
    "    rpy2.robjects.numpy2ri.activate()\n",
    "    r_random_seed = robjects.r['set.seed']\n",
    "    r_random_seed(random_seed)\n",
    "    rmclust = robjects.r['Mclust']\n",
    "\n",
    "    res = rmclust(rpy2.robjects.numpy2ri.numpy2rpy(adata.obsm[used_obsm]), num_cluster, modelNames)\n",
    "    mclust_res = np.array(res[-2])\n",
    "\n",
    "    adata.obs['mclust'] = mclust_res\n",
    "    adata.obs['mclust'] = adata.obs['mclust'].astype('int')\n",
    "    adata.obs['mclust'] = adata.obs['mclust'].astype('category')\n",
    "    return adata\n",
    "    \n",
    "def load_data(_dir):\n",
    "    feat_names = pd.read_csv(join(_dir, 'features.tsv.gz'), compression='gzip', sep='\\t', header=None)\n",
    "    barcodes   = pd.read_csv(join(_dir, 'barcodes.tsv.gz'), compression='gzip', sep='\\t', header=None)\n",
    "\n",
    "    with gzip.open(join(_dir, 'matrix.mtx.gz'), 'rb') as gzipped_file:\n",
    "        mat = mmread(gzipped_file)\n",
    "\n",
    "    ad = sc.AnnData(sps.csr_matrix(mat.T))\n",
    "    ad.obs_names = barcodes[0].values\n",
    "    ad.var_names = feat_names[1].values\n",
    "    ad.var['id'] = feat_names[0].values\n",
    "    ad.var['type'] = feat_names[2].values\n",
    "    return ad\n",
    "\n",
    "import json\n",
    "import copy\n",
    "from matplotlib.image import imread\n",
    "def flip_coords(ads):\n",
    "    for ad in ads:\n",
    "        ad.obsm['spatial'] = -1 * ad.obsm['spatial']\n",
    "        ad.obsm['spatial'] = ad.obsm['spatial'][:, ::-1]\n",
    "\n",
    "def reorder(ad1, ad2):\n",
    "    shared_barcodes = ad1.obs_names.intersection(ad2.obs_names)\n",
    "    ad1 = ad1[shared_barcodes].copy()\n",
    "    ad2 = ad2[shared_barcodes].copy()\n",
    "    return ad1, ad2\n",
    "\n",
    "def load_peak_expr(_dir):\n",
    "    data = sio.mmread(join(_dir, 'data.mtx'))\n",
    "    cname = pd.read_csv(join(_dir, 'barcode.csv'), index_col=0)['x'].to_list()\n",
    "    feat = pd.read_csv(join(_dir, 'feat.csv'), index_col=0)['x'].to_list()\n",
    "    ad = sc.AnnData(sps.csr_matrix(data.T))\n",
    "    ad.obs_names = cname\n",
    "    ad.var_names = feat\n",
    "    return ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19d8714e-be84-4896-8f6f-75574c3c536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_col2cat(ad, cols=[]):\n",
    "    for col in cols:\n",
    "        ad.obs[col] = ad.obs[col].astype('category')\n",
    "\n",
    "def unify_colors(queries, color_key, ref_color_dict):\n",
    "    for q in queries:\n",
    "        q.obs[color_key] = q.obs[color_key].astype('category')\n",
    "        q.uns[f'{color_key}_colors'] = [ref_color_dict[_] for _ in q.obs[color_key].cat.categories]\n",
    "    return queries\n",
    "\n",
    "def subset_ad(ad, subset_index):\n",
    "    ad = ad[subset_index].copy()\n",
    "    return ad\n",
    "\n",
    "def set_spatial(ad):\n",
    "    ad.obsm['spatial'] = ad.obs[['array_row', 'array_col']].values\n",
    "    ad.obsm['spatial'] = ad.obsm['spatial'][:, ::-1]\n",
    "    ad.obsm['spatial'][:, 1] = -1 * ad.obsm['spatial'][:, 1]\n",
    "    return ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6706ddae-3067-4647-b21c-7a4e01fa3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_zu(_dir):\n",
    "    zs = []\n",
    "\n",
    "    for fi in sorted(os.listdir(_dir)):\n",
    "        dfi = pd.read_csv(join(_dir, fi), header=None)\n",
    "        zs.append(dfi.values)\n",
    "    zs = np.vstack(zs)\n",
    "    z, u = zs[:, :-2], zs[:, -2:]\n",
    "    return z, u\n",
    "\n",
    "def wrap_warn_plot(adata, basis, color, **kwargs):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "        sc.pl.embedding(adata, basis=basis, color=color, **kwargs)\n",
    "\n",
    "def get_umap(ad, use_reps=[]):\n",
    "    for use_rep in use_reps:\n",
    "        umap_add_key = f'{use_rep}_umap'\n",
    "        sc.pp.neighbors(ad, use_rep=use_rep, n_neighbors=15)\n",
    "        sc.tl.umap(ad)\n",
    "        ad.obsm[umap_add_key] = ad.obsm['X_umap']\n",
    "    return ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f60e017-d0b4-4a17-b9de-7a0a0bbf67cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = '/disco_500t/xuhua/gitrepo/midas/result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bec33661-beda-4631-bda0-694a7a0f7584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuhua/xuhua_disco/miniforge3/envs/Squidpy/lib/python3.8/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/home/xuhua/xuhua_disco/miniforge3/envs/Squidpy/lib/python3.8/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/home/xuhua/xuhua_disco/miniforge3/envs/Squidpy/lib/python3.8/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/home/xuhua/xuhua_disco/miniforge3/envs/Squidpy/lib/python3.8/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/home/xuhua/xuhua_disco/miniforge3/envs/Squidpy/lib/python3.8/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n",
      "/home/xuhua/xuhua_disco/miniforge3/envs/Squidpy/lib/python3.8/site-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/disco_500t/xuhua/data/spatial_multi_omics/lymp_node/LN-2024-new/outs'\n",
    "\n",
    "ad3 = load_data(join(data_dir, 'filtered_feature_bc_matrix'))\n",
    "ad3_rna = ad3[:, ad3.var['type']=='Gene Expression'].copy()\n",
    "ad3_adt = ad3[:, ad3.var['type']=='Antibody Capture'].copy()\n",
    "load_spatial(join(data_dir, 'spatial'), ad3_rna)\n",
    "load_spatial(join(data_dir, 'spatial'), ad3_adt)\n",
    "\n",
    "ad3_rna.obs['src'] = ad3_adt.obs['src'] = ['s3']*ad3_rna.n_obs\n",
    "ad3_rna.obs_names = [f's3-{x}' for x in ad3_rna.obs_names]\n",
    "ad3_adt.obs_names = [f's3-{x}' for x in ad3_adt.obs_names]\n",
    "\n",
    "ad3_rna.var_names_make_unique()\n",
    "ad3_adt.var_names_make_unique()\n",
    "\n",
    "data_dir = '/disco_500t/xuhua/data/spatial_multi_omics/lymp_tonsil_ramen'\n",
    "\n",
    "ad_a1_rna = sc.read_h5ad(join(data_dir, 'lymph_A1/adata_RNA.h5ad'))\n",
    "ad_a1_adt = sc.read_h5ad(join(data_dir, 'lymph_A1/adata_ADT.h5ad'))\n",
    "meta1 = pd.read_csv(join(data_dir, 'lymph_A1/A1_LN_cloupe_Kwoh.csv'), index_col=0) \n",
    "ad_a1_rna.obs['lab'] = meta1.loc[ad_a1_rna.obs_names, 'manual'].to_list()\n",
    "ad_a1_adt.obs['lab'] = meta1.loc[ad_a1_adt.obs_names, 'manual'].to_list()\n",
    "ad_a1_rna.obs['src'] = ad_a1_adt.obs['src'] = ['s1'] * ad_a1_rna.n_obs\n",
    "ad_a1_rna.obs_names = [f's1-{x}' for x in ad_a1_rna.obs_names]\n",
    "ad_a1_adt.obs_names = [f's1-{x}' for x in ad_a1_adt.obs_names]\n",
    "ad_a1_rna.var_names_make_unique()\n",
    "ad_a1_adt.var_names_make_unique()\n",
    "\n",
    "ad_d1_rna = sc.read_h5ad(join(data_dir, 'lymph_D1/adata_RNA.h5ad'))\n",
    "ad_d1_adt = sc.read_h5ad(join(data_dir, 'lymph_D1/adata_ADT.h5ad'))\n",
    "meta2 = pd.read_csv(join(data_dir, 'lymph_D1/D1_LN_cloupe_Kwoh.csv'), index_col=0) \n",
    "ad_d1_rna.obs['lab'] = meta2.loc[ad_d1_rna.obs_names, 'manual'].to_list()\n",
    "ad_d1_adt.obs['lab'] = meta2.loc[ad_d1_adt.obs_names, 'manual'].to_list()\n",
    "ad_d1_rna.obs['src'] = ad_d1_adt.obs['src'] = ['s2'] * ad_d1_rna.n_obs\n",
    "ad_d1_rna.obs_names = [f's2-{x}' for x in ad_d1_rna.obs_names]\n",
    "ad_d1_adt.obs_names = [f's2-{x}' for x in ad_d1_adt.obs_names]\n",
    "ad_d1_rna.var_names_make_unique()\n",
    "ad_d1_adt.var_names_make_unique()\n",
    "\n",
    "## unify feature names\n",
    "shared_gene = ad_a1_rna.var_names.intersection(ad_d1_rna.var_names).intersection(ad3_rna.var_names)\n",
    "shared_prot = ad_a1_adt.var_names.intersection(ad_d1_adt.var_names).intersection(ad3_adt.var_names)\n",
    "\n",
    "ad_a1_rna, ad_d1_rna, ad3_rna = ad_a1_rna[:, shared_gene].copy(), ad_d1_rna[:, shared_gene].copy(), ad3_rna[:, shared_gene].copy()\n",
    "ad_a1_adt, ad_d1_adt, ad3_adt = ad_a1_adt[:, shared_prot].copy(), ad_d1_adt[:, shared_prot].copy(), ad3_adt[:, shared_prot].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7362372a-b576-4aac-ab5c-a092b3358b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_rna_all = sc.concat([ad_a1_rna, ad_d1_rna, ad3_rna])\n",
    "ad_adt_all = sc.concat([ad_a1_adt, ad_d1_adt, ad3_adt])\n",
    "\n",
    "sc.pp.highly_variable_genes(ad_rna_all, batch_key=\"src\", flavor=\"seurat_v3\", n_top_genes=5000)\n",
    "\n",
    "ad_a1_rna = ad_a1_rna[:, ad_rna_all.var.query('highly_variable').index].copy()\n",
    "ad_d1_rna = ad_d1_rna[:, ad_rna_all.var.query('highly_variable').index].copy()\n",
    "ad3_rna = ad3_rna[:, ad_rna_all.var.query('highly_variable').index].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "055bb43a-6fa4-4852-ae05-8cab857deb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNA_ADS = [ad_a1_rna, ad_d1_rna, ad3_rna]\n",
    "ADT_ADS = [ad_a1_adt, ad_d1_adt, ad3_adt]\n",
    "n_batches = 3\n",
    "mod_dict = {'rna':RNA_ADS, 'adt':ADT_ADS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31caae0c-5deb-4d02-9a97-e0fa8dde62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_batches):  # train test split\n",
    "    for mod in ['adt']:   # missing mod\n",
    "        tmp_out_dir = f'/disco_500t/xuhua/gitrepo/midas/data/processed/Lymph_cv{i+1}_missing{mod}'\n",
    "        print(tmp_out_dir)\n",
    "        feat_dir = join(tmp_out_dir, 'feat')\n",
    "        os.makedirs(feat_dir, exist_ok=True)\n",
    "         \n",
    "        df_feat_dims = pd.DataFrame(np.array([ad_a1_rna.n_vars, ad_a1_adt.n_vars]).reshape(1, -1), columns=['rna', 'adt'])\n",
    "        df_feat_rna_names = pd.DataFrame(ad_a1_rna.var_names, columns=['x'])\n",
    "        df_feat_adt_names = pd.DataFrame(ad_a1_adt.var_names, columns=['x'])\n",
    "        df_feat_dims.to_csv(join(feat_dir, 'feat_dims.csv'))\n",
    "        df_feat_rna_names.to_csv(join(feat_dir, 'feat_names_rna.csv'))\n",
    "        df_feat_adt_names.to_csv(join(feat_dir, 'feat_names_adt.csv'))\n",
    "\n",
    "        # # each subset\n",
    "        subsets, mods = [], []\n",
    "        for bi in range(3):\n",
    "            tmp_set, tmp_mod = [], []\n",
    "            for bmod in ['rna', 'adt']:\n",
    "                if (bi==i) and (bmod == mod):\n",
    "                    continue\n",
    "                tmp_set.append(mod_dict[bmod][bi])\n",
    "                tmp_mod.append(bmod)\n",
    "            subsets.append(tmp_set)\n",
    "            mods.append(tmp_mod)\n",
    "        \n",
    "        print(mods)\n",
    "        for si in range(3):\n",
    "            for fname in ['mask', 'mat', 'vec']:\n",
    "                os.makedirs(join(tmp_out_dir, f'subset_{si}/{fname}'), exist_ok=True)\n",
    "            tmp_dir = join(tmp_out_dir, f'subset_{si}')\n",
    "            for ad,mi in zip(subsets[si], mods[si]):\n",
    "                mat = ad.X.A if sps.issparse(ad.X) else ad.X\n",
    "                df_mat = pd.DataFrame(mat, index=ad.obs_names, columns=ad.var_names)\n",
    "                df_mat.to_csv(join(tmp_dir, f'mat/{mi}.csv'))\n",
    "        \n",
    "                os.makedirs(join(tmp_dir, f'vec/{mi}'), exist_ok=True)\n",
    "                for idx, mati in enumerate(mat):\n",
    "                    pd.DataFrame(mati.reshape(1, -1)).to_csv(join(tmp_dir, 'vec/{}/{:05d}.csv'.format(mi, idx)), header=None, index=None)\n",
    "                pd.DataFrame(ad.obs_names, columns=['x']).to_csv(join(tmp_dir, 'cell_names.csv'))\n",
    "        \n",
    "                # save mask\n",
    "                pd.DataFrame(np.ones(ad.n_vars, dtype='int')).to_csv(join(tmp_dir, f'mask/{mi}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f0101ee-e487-4c09-81a5-a4ab37d9326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "e, ep = 1, 2000\n",
    "for i in range(3):  # train test split\n",
    "    for mod in ['adt']:   # missing mod\n",
    "        training_command = f'CUDA_VISIBLE_DEVICES=1 python run.py --exp e{e} --task Lymph_cv{i+1}_missing{mod} --epoch_num {ep}'\n",
    "    \n",
    "        run_command = 'CUDA_VISIBLE_DEVICES=1 python run.py --task Lymph_cv{}_missing{} --act translate --init_model sp_{:08d} --exp e{}'\\\n",
    "                                    .format(i+1, mod, ep-1, e)\n",
    "        print(training_command)\n",
    "        print(run_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfb87885-79ae-46eb-b2f8-87a1aa26bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def csv_read(path):\n",
    "    res = []\n",
    "    with open(path, mode='r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "\n",
    "        for row in reader:\n",
    "            try:\n",
    "                float_row = [float(item) for item in row]\n",
    "                res.append(float_row)  # Each row is now a list of floats\n",
    "            except ValueError as e:\n",
    "                print(f\"Error converting to float: {e}\")\n",
    "    res = np.vstack(res)   \n",
    "    return res\n",
    "\n",
    "def collect_csv(_dir):\n",
    "    fls = sorted(os.listdir(_dir))\n",
    "    res = []\n",
    "    for fl in fls:\n",
    "        # df = pd.read_csv(join(_dir, fl), header=None)\n",
    "        data = csv_read(join(_dir, fl))\n",
    "        res.append(data)\n",
    "    res = np.vstack(res)\n",
    "    return res\n",
    "    \n",
    "import copy\n",
    "def binarize(Xs, bin_thr=0):\n",
    "    rs = []\n",
    "    for X in Xs:\n",
    "        X = copy.deepcopy(X.A) if sps.issparse(X) else copy.deepcopy(X)\n",
    "        X[X>bin_thr] = 1\n",
    "        rs.append(X)\n",
    "    return rs\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def eval_AUC_all(gt_X, pr_X, bin_thr=1):\n",
    "    gt_X = binarize([gt_X], bin_thr)[0].flatten()\n",
    "    pr_X = pr_X.flatten()\n",
    "    auroc = roc_auc_score(gt_X, pr_X)\n",
    "    return auroc\n",
    "\n",
    "def PCCs(gt_X, pr_X):\n",
    "    pcc_cell = [np.corrcoef(gt_X[i,:], pr_X[i,:])[0,1] for i in range(gt_X.shape[0])] \n",
    "    pcc_peak = [np.corrcoef(gt_X[:,i], pr_X[:,i])[0,1] for i in range(gt_X.shape[1])] \n",
    "    return pcc_cell, pcc_peak\n",
    "\n",
    "def cal_cmd(pred, true):\n",
    "    zero_rows_indices1 = list(np.where(~pred.any(axis=1))[0]) # all-zero rows\n",
    "    zero_rows_indices2 = list(np.where(~true.any(axis=1))[0])\n",
    "    zero_rows_indices = zero_rows_indices1 + zero_rows_indices2\n",
    "    rm_p = len(zero_rows_indices) / pred.shape[0]\n",
    "    if rm_p >= .05:\n",
    "        print(f'Warning: two many rows {rm_p}% with all zeros')\n",
    "    pred_array = pred[~np.isin(np.arange(pred.shape[0]), zero_rows_indices)].copy()\n",
    "    true_array = true[~np.isin(np.arange(true.shape[0]), zero_rows_indices)].copy()\n",
    "    corr_pred = np.corrcoef(pred_array,dtype=np.float32)\n",
    "    corr_true = np.corrcoef(true_array,dtype=np.float32)\n",
    "    \n",
    "    x = np.trace(corr_pred.dot(corr_true))\n",
    "    y = np.linalg.norm(corr_pred,'fro')*np.linalg.norm(corr_true,'fro')\n",
    "    cmd = 1- x/(y+1e-8)\n",
    "    return cmd\n",
    "\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "824e6153-9ffe-4895-9cd3-e1c0381e68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/disco_500t/xuhua/gitrepo/midas/result'\n",
    "for i in range(3):\n",
    "    pr_atac = collect_csv(join(out_dir, f'Lymph_cv{i+1}_missingadt/e1/default/predict/sp_00001999/subset_{i}/x_trans/rna_to_adt'))\n",
    "    gt_atac = ADT_ADS[i].X.A if sps.issparse(ADT_ADS[i].X) else ADT_ADS[i].X\n",
    "\n",
    "    ad_pred = sc.AnnData(pr_atac, obs=ADT_ADS[i].obs.copy())\n",
    "    ad_pred.var_names = ADT_ADS[i].var_names.copy()\n",
    "    ad_pred.write_h5ad(f'/disco_500t/xuhua/gitrepo/BridgeNorm/figures/imputation/Lymph/midas/cv{i+1}_imputedADT.h5ad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeefc30-5e43-4819-8426-71657e125f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Squidpy",
   "language": "python",
   "name": "squidpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
